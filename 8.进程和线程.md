## 进程和线程

>   -   对于操作系统来说，一个任务就是一个进程（Process） 
>   -   在一个进程内部，要同时干多件事，就需要同时运行多个“子任务”，我们把进程内的这些“子任务”称为线程（Thread）。 

 我们前面编写的所有的Python程序，都是执行单任务的进程，也就是只有一个线程 ,当`python` 需要执行多任务时我们可以使用:

-   多进程模式 ----> 启动多个含一个线程的进程
-   多线程模式 ----> 启动一个含多线程的进程



###### 多进程与多线程的区别

>   多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响
>
>   而多线程中，所有变量都由所有线程共享，所以，任何一个变量都可以被其中任何一个线程修改 



### 1.多进程

###### 1.使用 `Process` 创建少量进程:

例:

```python
'''现在我们开始创立多线程'''
from multiprocessing import Process
import os
def run_child_process(name): #我们创立子进程要执行的程序
    print(name,os.getpid()) #可以理解为拿到子进程的身份证
    pass

if __name__ == '__main__': #必须有
    print('the father card is %s' % os.getpid()) #为拿到父进程的通行证
    pa= Process(target = run_child_process,args = (1))
    pb= Process(target = run_child_process,args = (2))
    pa.start(),pb.start() #启动子进程
    pa.join(),pb.join() #等待子进程结束
    print('child process end')
    
'''现在我们在 cmd 命令行来试着运行下这个程序'''
$ the father card is 983
1 4566
2 9808
child process end 
```



###### 2.要启动大量子进程,我们可以采用 `Pool` 方法:

例:

```python
from multiprocessing import Pool
import os, time, random

def child_process(name):
    print(name,os.getpid())
    time.sleep(0.5)
    print(name,'have done')

if __name__=='__main__':
    print('father process %s.' % os.getpid())
    p = Pool(4)
    for i in range(5): #这里故意添加超出 进程池 容量的进程
        p.apply_async(child_process, args=(i,)) # , 不能删,表示元组
  #往进程池里添加子进程
    p.close() #关闭进程池,无法再添加进程了
    p.join() #开始运行并等待子进程运行完毕,注意区分此方法与 Process 的方法的区别
    print('All subprocesses done.')
    
'''现在我们试着运行一下'''
$ father process 590
0 9768
1 8907
2 9760
3 9699
0 have down #由于 Pool 容量仅有4个,所以只能同时跑4个进程,故第五个进程需要等待其中一个进程完毕,才能运行
4 5906
..... #这里省略
```



###### 3.使用 `subprocess` 直接启用一个外部进程:

```python
'''
这段代码与在命令行输入`nslookup www.python.org` 一样
'''
import subprocess
print(subprocess.call(['nslookup','www.python.org']))
'''
当我们还需在外部进程再写上几行命令时,还可以使用 `communicate` 方法输入,这里就不详述了
'''
```



###### 4.使用`Queue`进行进程间的通信

例:

```python
from multiprocessing import Queue,Process
import time
def do_it(que):
    print(que.get())
    pass
if __name__ == '__main__':
    que = Queue()
    a = Process(target = do_it,args=(que,))
    que.put('i like you')
    a.start()
    a.join()
'''现在让我们来看看效果'''
$ i like you #说明成功完成进程通信
```



### 2.多线程

>   `python` 由于有 GIL 全局锁的限制 , 故仅能占用一个CPU完成多线程操作

###### 1.使用`Thread` 创建多线程:

```python
'''其与创建多进程非常类似'''
import threading
def child_thread_doing():
    print(threading.current_thread().name)
    pass
'''现在我们开始创造线程'''
print(threading.current_thread().name)#调出父线程名字
t1 = threading.Thread(target=child_theread_doing)
t2 = threading.Thread(target=child_thread_doing,name='child_thread')
t1.start();t2.start()
t1.join();t2.join()

'''现在我们试着运行一下'''
MainThread
Thread-1 #当不用调用 name 参数时,程序默认子线程为Thread-number
child_thread
```



###### 2.使用`Lock`方法,让同一时间仅有一个线程工作:

>   在多线程环境下，为了能有效避免线程间互相胡乱修改变量 ,我们需要对于所有全局变量加锁,

闲话少说,码代码:

```python
'''其与创建多进程非常类似'''
import threading
lock = threading.Lock()
def child_thread_doing():
    lock.acquire() #将该线程锁上,仅允许它工作
    print(threading.current_thread().name)
    lock.release() #将锁打开,让以便让其他线程上锁,请牢记,如果没有这步程序将一直等待
.......
```



###### 3.使用`ThreadLocal` 调用局部变量

>     多线编程下 , 为了让线程能够单独修改仅属于自己的变量 , 我们还需要为每个线程配置仅属于他们的局部变量 
>
>   但一般情况下将全局变量转换为每个线程的局部变量都比较繁琐,这时候我们就可以调出神器 `ThreadLocal`了. 

举例如下:

```python
import threading
#创建承载各局部对象的 "全局对象" 对象
local_school = threading.local()

def child_thread(name):
    local_school.student_name = name #绑定一个局部对象
    std = local_school.student_name #获取局部对象
    print('Hello %s (in %s)' % (std,threading.current_thread().name))
    pass

'''我们现在开始创立子线程'''
t1 = threading.Thread(target= child_thread, args=('Alice',))
t2 = threading.Thread(target= process_thread, args=('Bob',))
t1.start();t2.start()
t1.join();t2.join()

'''让我们来看看这方法的威力'''
Hello, Alice (in Thread-1)
Hello, Bob (in Thread-2) #看来各局部变量地创立并应用了
```

`ThreadLocal`最常用的地方就是为每个线程绑定一个数据库连接，HTTP请求，用户身份信息等



### 3.进程 vs 线程

>   多进程的优点是稳定性高
>
>   多线程的优点是运行效率快

>   故为了让服务器同时拥有以上优点,有许多公司开始采用`多进程加多线程` 的模式 , 但其缺点是编写代码过于反人类   
>



###### 1.进程切换

>   ` cpu`执行指令需要寄存器，切换任务首先需要操作系统保存当前寄存器的现场，然后恢复上次的寄存
>
>   虽然上述过程执行很快,但如果同时有大量进程同时进行,操作系统可能处于一直切换任务的状态, 这种情况最常见的就是点窗口无反应，系统处于假死状态 



###### 2.计算密集型 vs IO密集型

>    计算密集型任务的特点是 : 要进行大量的计算，消耗CPU资源,如计算圆周率,故为了最高效地利用 `cpu` ,该任务同时进行的数量应当等于CPU的核心数 
>
>   IO密集型任务的特点是 :  CPU消耗很少，任务的大部分时间都在等待IO请求操作完成 ,如读写文件,web访问 , 故理应我们应该采用多进程或多线程来高效利用`cpu`,但现在我们还有更好的解决方法:
>
>   结合操作系统提供的异步IO支持 , 就可以利用单进程或单线程模型来执行多任务,  该方法对应到`python` 就是 **协程**



### 4.分布式进程

>    在Thread和Process中，应当优选Process，因为Process更稳定，而且，Process可以分布到多台机器上，而Thread最多只能分布到同一台机器的多个CPU上
>
>   故我们采用`Process` 来进行分布式进程,进行我们分布式进程的模块就是 `multiperocess` 下的 `manager` 模块



现在我们先写出**分配模块**的代码:

```python
#利用queue模块进行计算机间的通信
import queue 

from multiprocessing.managers import BaseManager
 
from multiprocessing import freeze_support 
#windows下多进程可能会炸,故我们引入freeze_support函数来缓解这种情况

'''初始化各参数'''
#获取发送与接收任务的队列
task_queue = queue.Queue()
result_queue = queue.Queue()

#将BaseManager重新封装到一个类上
class QueueManager(BaseManager):
    pass

#将队列封装到函数里,注意:windows 用户才需要这样,linux用户仅需动用lambda即可
def func_task_queue():
    return task_queue   
def func_result_queue():
    return result_queue

'''将所有具体操作放到一个函数里,使用时是让后面的 if "__name__" = __main__ 确认成功后使用,具体原因我也不清楚'''
def do_task_master():
    #由于QueueManager管理的不止一个Queue，所以，要给每个Queue的网络调用接口起个名字,然后就可以将函数封装的两个 Queue 都注册到网上
    
    QueueManager.register('get_task_queue',callable = func_task_queue)
    QueueManager.register('get_result_queue',callable = func_result_queue)
    
    #链接到本地服务器,并绑定端口5000,设置二进制处理后的验证码 abc,并将这个全部设置好了的 QueueManager 对象传给 manager变量
    manager = QueueManager(address = ('127.0.0.1',5000),authkey = b'abc')
    
    #启动 Queue
    manager.start()

    #通过 网络 获取 Queue 对象
    task = manager.get_task_queue()
    result = manager.get_result_queue()

    ''' res5redsz	
    终于将前期配置完成咯,现在我们开始操作啦!!!
    '''
    #我们先将一些东西放进网络里的 Queue 队列
    the_list = ['hello','world']
    for i in the_list:
        print('I will input %s' % i)
        task.put(i)
        pass

    #我们再将处理模块返回到网络的 result 对象提取
    print('we are trying to get results')
    for i in range(len(the_list)):
        #我们使用try except 来捕获 r 为空的情况
        try:
            r = result.get(timeout = 10)
            print('haha,we get the %s' % r)
        except queue.Empty:
            print('oh,sorry',i,'of the result is empty')
        #最后我们调用 finally 让程序保证一定能执行到这一步,并在后面输出一小段话,让我们知道程序正常关闭了
        finally:
            manage.shutdown()
            print('the manage has shuted in right way')

if __name__ == '__main__':
    freeze_support() #缓解在多线程Windows环境下系统的压力
    do_task_master()
```



现在我们尝试写一写**分配模块**的代码吧!!!

```python
import time,queue
from multiprocessing.managers import BaseManager

class QueueManager(BaseManager):
    pass

def do_task_work():
    #因为我们仅需从网络上得到 Queue 队列,故不用创建队列了
    QueueManager.register('get_task_queue')
    QueueManager.register('get_result_queue')
    
    #我们查找的地址,端口,验证码,应与处理模块设置的一致
    manager = QueueManager(address=('127.0.0.1',5000),authkey = b'abc')
    
    #现在我们连接在处理模块中创建的端口下的地址
    manager.connect()
    
    #我们现在取得封装在 get_task_queue 与 get_result_queue
    #里的 Queue 队列
    task = manager.get_task_queue()
    result = manager.get_result_queue()
    
    '''现在我们开始操作了'''
    the_list = ['hello','world']
    for i in range(len(the_list)):
        try:
            n = task.get(timeout=1)
            print('we have get the',n)
            time.sleep(1)
            process_n = 'i have process the ' + n
            result.put(process_n)
        except queue.Empty:
            print("we can't get the range of",i)
        finally:
            print('worker have down')
            
if __name__ == '__main__':
    do_task_work()
```



好了终于将 **分配模块** 与 **处理模块** 编写好了,现在我们试着运行一下它们:

```python
#注意:我们应该在两个 cmd 命令行下先后运行分配模块和处理模块
#因为在一个 cmd 命令行运行的话,后运行的模块会使先运行的模块强行停止,导致 manager.shutdown() 未执行,下次运行时会出现管道已关闭的错误
'''好了我们来看看这两个模块的威力吧'''
$ we input the hello
we input the world
we are trying to get the result
i have get the hello
i have get the world
worker have down
haha,we get the i have process hello
haha,we get the i have process world
the manage shuted in right way
```

这就是分布式计算, 我们还可以把代码稍加改造，启动多个worker，就可以把任务分布到几台甚至几十台机器上 , 使得原本可能让一台计算机不堪重负的任务 , 分配出来到许多台计算机 , 以提高执行效率.

